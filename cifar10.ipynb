{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PKgXo17x3_hg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import models.cifar10CNN\n",
    "import models.WaveletCifar10CNN\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_9maP4z3_C8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TfvnzkdZSqli"
   },
   "outputs": [],
   "source": [
    "weights_filepath = 'weights'\n",
    "logs_filepath = 'logs'\n",
    "\n",
    "\n",
    "if not os.path.exists(weights_filepath):\n",
    "    os.makedirs(weights_filepath)\n",
    "\n",
    "if not os.path.exists(logs_filepath):\n",
    "    os.makedirs(logs_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "i7Y0GE3jSqlj"
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "num_folds = 10\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "lr = 1e-4  # learning rate\n",
    "beta_1 = 0.9         # beta 1 - for adam optimizer\n",
    "beta_2 = 0.96        # beta 2 - for adam optimizer\n",
    "epsilon = 1e-7        # epsilon - for adam optimizer\n",
    "\n",
    "trainFactor = 0.8\n",
    "imageShape = (32, 32, 3)  # CIFAR-10 60,000 32X32 color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "I9Sur-BnSqlj"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tc2eXRu6Sqlj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Olya\\Anaconda3\\envs\\dwtcnn\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)  # SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WHTWJQwwSqlj"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BS6tyzZwSqlk",
    "outputId": "9f924190-9aa6-451c-9d52-a0528ae34b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Olya\\Anaconda3\\envs\\dwtcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Olya\\Anaconda3\\envs\\dwtcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      " - 22s - loss: 2.1645 - accuracy: 0.2488 - val_loss: 1.6002 - val_accuracy: 0.4183\n",
      "Epoch 2/30\n",
      " - 21s - loss: 1.7036 - accuracy: 0.3768 - val_loss: 1.5140 - val_accuracy: 0.4545\n",
      "Epoch 3/30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and Train the Model\n",
    "model = models.cifar10CNN.cifar10CNN(imageShape, nb_classes)\n",
    "history_file_path = \"trainHistoryCifar10CNN.txt\"  # save loss and val loss\n",
    "\n",
    "# model = models.WaveletCifar10CNN.WaveletCNN(imageShape, nb_classes)\n",
    "# history_file_path = \"trainHistoryWaveletCifar10CNN.txt\" # save loss and val loss\n",
    "# model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "h5_tmp = \"tmp.h5\"\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_split=1 - trainFactor,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=2,\n",
    "                    callbacks=[\n",
    "                        ModelCheckpoint(h5_tmp, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n",
    "                        EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\n",
    "                        ],\n",
    "                    )\n",
    "\n",
    "model.load_weights(h5_tmp)\n",
    "weights_path = os.path.join(weights_filepath, \"WCNNN.h5\")\n",
    "model.save(weights_path)\n",
    "\n",
    "# Model Evaluation\n",
    "result = model.evaluate(x_test, y_test)\n",
    "\n",
    "with open(history_file_path, 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR7psEeUSqll"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_file_path = r\"trainHistoryWaveletCifar10CNN.txt\"\n",
    "\n",
    "with open(history_file_path, 'rb') as pickle_file:\n",
    "    history = pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "# plot train and validation loss\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "cifar10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
